\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{fancyhdr} %per personalizzare l'intestazione
\usepackage[italian]{babel}
\usepackage{tabularx}
\usepackage{lscape}
\usepackage[title]{appendix}
\usepackage{rotating}
\usepackage[margin=2.0cm]{geometry}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage{wrapfig} %immagine all'interno del testo
\graphicspath{ {figures/} }
\usepackage{graphicx}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{amsmath}
\addtolength{\topmargin}{-1cm}
\addtolength{\textheight}{1.1cm}

\usepackage{float}%per forzare la posizione di un'immagine (H)
\usepackage{subfig}

\pagenumbering{roman} %pagine iniziali con numeri romani

%for numbering figures and tables with section
%\usepackage{chngcntr}
%\counterwithin{figure}{section}
%\usepackage{chngcntr}
%\counterwithin{table}{section}

\title{Report Data Mining II}
\author{Ferri Lorenzo (607828) \\ email \href{mailto:lorenzoferri1995@gmail.com}{lorenzoferri1995@gmail.com} \and Pappolla Roberta (534109) \\ email
 \href{mailto:r.pappolla@studenti.unipi.it}{r.pappolla@studenti.unipi.it} \and Ema Ilic (602796) \\ email \href{mailto:e.ilic@studenti.unipi.it}{e.ilic@studenti.unipi.it}}
\date{Data Mining (654AA), Anno accademico 2019/2020}

\begin{document}
\maketitle
\newpage
\tableofcontents

\newpage
\listoffigures
\listoftables
\newpage

\pagenumbering{arabic} %conversione in numeri arabi


\section{Task 1}

\subsection{Data Understanding and Preparation}

Il Training Set fornito è costituito da 8143 Oggetti e 7 Attributi. 'Occupancy' è l'attributo binario in output su cui deve essere eseguita la classificazione, che riguarda la previsione della presenza o meno di un soggetto in una stanza. 'Date' è il momento temporale in cui vengono rilevati i valori dei restanti attributi, che sono tutti numerici. La serie temporale riguarda 6 giorni, dal 2015-02-04 alle 17:51:00 al 2015-02-10 alle 09:33:00, con una frequenza di rilevazione di un minuto. Il dataset non presenta alcun Missing Value e gli unici Null Values presenti sono 5160 valori dell'attributo 'Light'. Questi non sembrano costituire un errore di rilevazione perché sono coerenti con il significato dell'attributo (luce nulla quando la stanza è chiusa e inutilizzata o nelle ore notturne). Tutti gli attributi numerici presentano un picco di frequenze sulla parte bassa dei valori, caratteristica questa che è decisamente accentuata negli attributi 'Light' e 'CO2'. E' possibile che 0 e 450 circa siano rispettivamente i valori di 'Light' e 'CO2' nella stanza quando nessuno è presente. Valori diversi invece potrebbero indicare una certa attività nella stanza. Tale ipotesi è avvalorata dai Box Plot dei due attributi con valori raggruppati per classe in output. Le due mediane dei valori di 'Light'e 'CO2' quando 'Occupancy' è 1 sono rispettivamente 450 e 950 circa, molto diversi dai picchi in distribuzione.

\begin{figure}[H] \centering
\includegraphics[width=7cm]{data_understanding/Light Histogram.png}
\includegraphics[width=6cm]{data_understanding/Light Boxplot by Occupancy.png}

\includegraphics[width=7cm]{data_understanding/CO2 Densities.png}
\includegraphics[width=6cm]{data_understanding/CO2 Boxplot by Occupancy.png}

\caption{\label{fig:frog1}Distribuzioni e Box Plot 'Light', 'CO2'}
\end{figure}

\begin{wrapfigure}[10]{r}{0.5\textwidth}
\includegraphics[width=7cm]{data_understanding/Light vs Temperature Scatterplot.png}
\end{wrapfigure}

\noindent Sono stati riprodotti gli Scatter Plot di ciascuna coppia di attributi numerici. I due attributi con la maggior dipendenza lineare sono ovviamente risultati 'Humidity' ed 'Humidity Ratio'. Di fatto i due attributi esprimono lo stesso concetto, l'umidità, in due formati diversi (assoluta e relativa). Sebbene fosse possibile decidere di eliminare uno di essi ciò non è stato fatto perché la complessità del dataset non è un problema. Da un primo elementare approccio grafico l'unico attributo che riesce a discriminare bene tra le classi in output è 'Light'. L'attributo che ha mostrato la correlazione più elevata con 'Light' è stato 'Temperature': Pearson Corr = 0.65, Spearman Corr = 0.565, Kendall Corr = 0.415.


\subsection{Basic Classification Methods}

Lo scopo di questi modelli non è classificare una Time Series, quindi l'attributo relativo al tempo, 'date', è stato eliminato. Nei modelli in cui risultasse necessario sono stati cercati i migliori Iper-Parametri mediante una Random Search che ottimizzasse il Positive F1 Score medio che esce dalle 5 iterazioni compiute dalla Cross-Validation sul Training Set (Random Search CV con 5 Fold). La decisione dell'F1 come metrica da ottimizzare deriva dalla maggior attenzione riservata alle performance in termini di Precision e Recall per la classe 1 in output. I Decision Boundary sono stati rappresentati fittando il modello sulle due dimensioni rappresentate (non rappresenta dunque il reale Boundary su tutto il dataset ma ne fornisce un'approssimazione).
\\

\noindent \textbf{Decision Tree}: La Random Search CV è stata eseguita più volte cercando la combinazione più efficace degli Iper-Parametri 'min samples split' e 'min samples leaf' in un dominio molto ampio per entrambi [1, 500]. Le performance migliori si hanno per valori alti degli Iper-Parametri, che portano ad alberi semplici. L'albero migliore è uno split sull'attributo 'Light' al valore 365.125: le istanze classificate positive sono quelle con 'Light' > 365.125. L'attributo 'Light' quindi riveste un'importanza totale nella classificazione.

\begin{figure}[H] \centering
\includegraphics[width=7cm]{basic_classification/Decision Tree Plot.png}
\includegraphics[width=9cm]{basic_classification/Decision Boundary Decision Tree CO2 vs Light.png}

\caption{\label{fig:frog1}Decision Tree Results}
\end{figure}

\noindent Gli oggetti misclassificati sul Training Set sono solo 99 (di cui 90 sulla classe positiva). La 5 Fold Cross-Validation sul Training Set ha restituito: Accuracy con confidenza del 95\% = 0.98 (+/- 0.05); F1 Score medio pesato con confidenza del 95\% = 0.98 (+/- 0.04). Un singolo Decision Boundary lineare è sufficiente a discriminare bene nelle classi in output e gli errori non sono aree densamente popolate. E' stato appurato che Decision Tree più complessi risultano overfittati sul Training Set e non performano bene sui Test Set.

\begin{center}
\begin{tabular}{ | m{13em} | m{3cm}| m{3cm} | } 
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
Accuracy: & 0.9786 & 0.9931 \\
\hline \centering
F1-score: & [0.9829, 0.9714] & [0.9956, 0.9838] \\ 
\hline \centering
Weighted Avg F1-score: & 0.9787 & 0.9931 \\ 
\hline \centering
Precision: & [1.00, 0.95] &  [1.00, 0.97] \\
\hline \centering
Recall: & [0.97, 1.00] & [0.99, 0.99] \\
\hline
\end{tabular}
\end{center}

\noindent L'AUC (Area sotto la ROC Curve) per i due Test Set è: AUC(Test 1)=0.983 e AUC(Test 2)=0.994. \newline
L'ipotesi formulata inizialmente in fase di Data Understanding per l'attributo 'Light' sembra essere confermata. Esso è sufficiente a predire la classe in output, con un semplice Stump che divide i casi in cui la stanza è lievemente o per niente illuminata ('Light' <= 365.125), caso tipico di assenza di persone all'interno, dai casi in cui la stanza è apprezzabilmente illuminata, perché ad esempio le finestre sono state aperte o è stata accesa la luce da qualcuno. Nel Training Set sono presenti 5160 oggetti con valore dell'attributo 'Light' pari a 0, tutti questi hanno valore della classe 'Occupancy' pari a 0.
\\

\noindent\textbf{K-NN}: Poiché il modello si basa sul calcolo delle distanze tra gli oggetti sia il Training che il Test Set sono stati Standardizzati (gli attributi sono stati riportati tutti a media 0 e varianza 1). La Random Search è stata eseguita cercando i migliori valori del numero di punti nelle vicinanze degli oggetti da classificare, nel dominio [1, 50], e del peso da dare a questi punti per classificare l'istanza. La miglior combinazione di Iper-Parametri è 'weights'='uniform', 'n neighbors'=46. 

\begin{figure}[H] \centering
\includegraphics[width=7cm]{basic_classification/Feature Importance K-NN.png}
\includegraphics[width=9cm]{basic_classification/Decision Boundary K-NN CO2 vs Light.png}
\end{figure}

\noindent L'attributo più importante è sempre 'Light' ma anche 'CO2' riveste una seppur minima importanza. La Cross-Validation sul Training Set ha restituito una performance peggiore di quella del Decision Tree: Accuracy con confidenza del 95\% = 0.95 (+/- 0.08); F1 Score medio pesato con confidenza del 95\% = 0.95 (+/- 0.07). Anche la performance sui Test Set è peggiore, quindi 'CO2' in realtà non aggiunge significatività al modello.

\begin{center}
\begin{tabular}{ | m{13em} | m{3cm}| m{3cm} | } 
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
Accuracy: & 0.9595 & 0.956 \\
\hline \centering
F1-score: & [0.9684, 0.9434] & [0.9721, 0.8957] \\ 
\hline \centering
Weighted Avg F1-score: & 0.9593 & 0.9561 \\ 
\hline \centering
Precision: & [0.96, 0.96] &  [0.97, 0.89] \\
\hline \centering
Recall: & [0.98, 0.93] & [0.97, 0.90] \\
\hline
\end{tabular}
\end{center}

\noindent L'AUC delle ROC Curve è rispettivamente 0.99 e 0.988 per i due Test Set.
\\

\noindent\textbf{Naive Bayes}: Non è necessario il Tuning di nessun iper-parametro.

\begin{wrapfigure}[10]{r}{0.5\textwidth}
\includegraphics[width=9cm]{basic_classification/Decision Boundary Naive Bayes CO2 vs Light.png}
\end{wrapfigure}

\noindent Come per il K-NN oltre a 'Light' anche la 'CO2' riveste una minima importanza e di nuovo i risultati in termini di Cross-Validation sul Training Set e di test sui Test Set sono lievemente peggiori dello Stump banale. Solo l'area sotto le ROC Curve è maggiore. Questo significa che usando il 50\% come threshold di probabilità per la classificazione il modello ha una performance peggiore ma in genere risulta più stabile del Decision Tree perché i True Positive e False Positive restituiscono risultati migliori se facciamo variare il threshold. \newline
AUC(Test 1)=0.989, AUC(Test 2)=0.996.
\\ \\


\begin{center}
\begin{tabular}{ | m{13em} | m{3cm}| m{3cm} | } 
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
Accuracy: & 0.9774 & 0.9876 \\
\hline \centering
F1-score: & [0.9820, 0.9699] & [0.9921, 0.9711] \\ 
\hline \centering
Weighted Avg F1-score: & 0.9776 & 0.9876 \\ 
\hline \centering
Precision: & [1.00, 0.95] &  [1.00, 0.95] \\
\hline \centering
Recall: & [0.97, 0.99] & [0.99, 0.99] \\
\hline
\end{tabular}
\end{center}



\begin{figure}[H] \centering
\includegraphics[width=6cm]{basic_classification/ROC Curve Test 1 Naive Bayes.png}
\includegraphics[width=6cm]{basic_classification/ROC Curve Test 2 Naive Bayes.png}
\caption{\label{fig:frog1}Naive Bayes Test on Test Set 1 and Test Set 2}
\end{figure}



\noindent\textbf{Logistic Regression}: Non è necessario il Tuning di nessun iper-parametro. Il modello ottenuto è il seguente:

\begin{equation}
P = \frac{1}{1 + \text{exp}(19.32 - 1.4 * \text{Temp} - 0.04 * \text{Humid} + 0.02 * \text{Light} + 0.01 * \text{CO2} - 0.1 * \text{HumidRatio})}
\end{equation}

\noindent Si assiste ad un incremento ulteriore dell'importanza di 'CO2' e ad una minima importanza anche dell'attributo 'Temperature'. 


\begin{figure}[H] \centering
\includegraphics[width=7cm]{basic_classification/Feature Importance Logistic Regression.png}
\includegraphics[width=9cm]{basic_classification/Decision Boundary Logistic Regression CO2 vs Light.png}
\caption{\label{fig:frog1}Decision Tree Validation}
\end{figure}

\noindent La Cross-Validation sul Training Set ha restituito valori leggermente migliori del Decision Tree, perché la varianza è minore: Accuracy con confidenza del 95\%: 0.98 (+/- 0.03); F1 Score medio pesato con confidenza del 95\%: 0.98 (+/- 0.03). Le performance sui Test Set sono ancora leggermente peggiori dello Stump ma l'AUC è risultata la più alta di tutti i modelli, rendendo di fatto la Logistic Regression il modello più stabile.

\begin{center}
\begin{tabular}{ | m{13em} | m{3cm}| m{3cm} | } 
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
Accuracy: & 0.9764 & 0.9842 \\
\hline \centering
F1-score: & [0.9811, 0.9683] & [0.9900, 0.9619] \\ 
\hline \centering
Weighted Avg F1-score: & 0.9765 & 0.9841 \\ 
\hline \centering
Precision: & [0.99, 0.95] &  [0.99, 0.97] \\
\hline \centering
Recall: & [0.97, 0.99] & [0.99, 0.95] \\
\hline
\end{tabular}
\end{center}

\begin{figure}[H] \centering
\includegraphics[width=7cm]{basic_classification/ROC Curve Test 1 Logistic Regression.png}
\includegraphics[width=7cm]{basic_classification/ROC Curve Test 2 Logistic Regression.png}
\caption{\label{fig:frog1}Logistic Regression Test on Test Set 1 and Test Set 2}
\end{figure}

\noindent La regressione logistica è stata calibrata e testata anche con il solo attributo 'Light' in input. Il modello ottenuto è il seguente:

\begin{tabular}{ m{20em} m{3cm} }
\begin{equation}
P = \frac{1}{1 + \text{exp}(0.025 - 8.752 * \text{Light})}
\end{equation}
&
\includegraphics[width=7cm]{basic_classification/Logistic Regression con 'Light'.png}
\end{tabular}

\noindent Le performance sui Test sono identiche a quelle del Decision Tree iniziale perché questo modello porta allo stesso Decision Boundary singolo.



\subsection{Regression}

Poiché 'Light' è risultato ampiamente l'attributo più importante è stato scelto per tentare di predirne i valori mediante le regressioni. E' stata calibrata una regressione lineare (in due dimensioni) con solo l'attributo 'Temperature' in input, che è quello risultato più correlato con 'Light' nella fase di Data Understanding. La regressione rappresentata è sul Test Set e sono stati eliminati gli oggetti con valore di 'Light' pari a 0 (solo graficamente, non nel calcolo dei risultati, per focalizzare l'attenzione sulla parte importante della relazione lineare).


\begin{tabular}{ m{20em} m{3cm} }
\begin{equation}
\text{Light} = -2447.03 + 124.47 * \text{Temp}
\end{equation}
&
\includegraphics[width=8cm]{regression/Light vs Temperature Linear Regression.png}
\end{tabular}


\noindent La performance viene misurata con il Coefficiente di Determinazione, lo Scarto Quadratico Medio e lo Scarto Assoluto Medio su entrambi i Test Set.

\begin{center}
\begin{tabular}{ | m{3em} | m{2cm}| m{2cm} | } 
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
R2: & 0.512 & 0.444 \\
\hline \centering
MSE: & 30530.250 & 24109.752 \\ 
\hline \centering
MAE: & 152.073 & 131.116 \\ 
\hline
\end{tabular}
\end{center}

\noindent E' stata prodotta la regressione multipla aggiungendo tutti gli altri attributi numerici in input. Il modello è stato regolarizzato per tentare di ridurne la complessità, ponendo più vicini a zero i parametri che non migliorano sensibilmente la predizione dell'output. Il miglior risultato è stato raggiunto con la Ridge Regression, della quale si riportano la funzione e i risultati sui Test Set.

\begin{equation}
\text{Light} = -1176.14 + 0.35 * \text{CO2} -5.78  * \text{Humid} -15.83  * \text{HumidRatio} +59.83  * \text{Temp}
\end{equation}


\begin{center}
\begin{tabular}{ | m{3em} | m{2cm}| m{2cm} | }
\hline
& Test Set 1 & Test Set 2 \\
\hline\hline \centering
R2: & 0.595 & 0.153 \\
\hline \centering
MSE: & 25364.214 & 36724.966 \\ 
\hline \centering
MAE: & 131.360 & 148.516 \\ 
\hline
\end{tabular}
\end{center}

\noindent Rispetto alla regressione lineare semplice la performance migliora di poco sul primo Test Set ma peggiora di molto sul secondo. Quindi gli attributi diversi da 'Temperature' non aiutano a migliorare la predizione su 'Light'.


\subsection{Dimensionality Reduction}

\noindent\textbf{Dimensionality Reduction Tests}: We addressed the task by first testing the various Dimensionality Reduction methods using  the Decision Tree as a classifier. After obtaining the results which we will discuss shortly,  we have tested various classification methods (namely, Decision Tree, KNN, Naïve Bayes and Logistic Regression) on two-component PCA (next paragraph). We started with the Variance Threshold method. We realized that, as expected, the different threshold parameters yielded different numbers of dimensions. Namely, in terms of integers, results of the parameter search can be observed in the adjacent table.


\begin{center}
\begin{tabular}{ | m{7em} | m{5cm}| }
\hline
Dataset Shape & Integer Variance Threshold \\
\hline\hline \centering
(8143,5) & 0 \\
\hline \centering
(8143,4) & 1 \\ 
\hline \centering
(8143,3) & 2 \\ 
\hline \centering
(8143,2) & 31 \\
\hline \centering
(8143,1) & 37926 \\
\hline
\end{tabular}
\end{center}



\noindent The results of the Variance threshold analysis are exactly the same in terms of precision, accuracy and recall as if the dataset dimensions were not reduced at all when the threshold is anywhere between 1 and 37925 (which yields a number of dimensions between 2 and 4). However, the results significantly worsen for number of dimensions equal to one which is achieved with a threshold of 37926 onwards(Figure 1).

\begin{figure}[H] \centering
\includegraphics[width=10cm]{dimens_reduction/figure_1.png}
\caption{\label{fig:frog1}Figure 1}
\end{figure}

\noindent Univariate Feature Selection, Singular Value Decomposition  and Principal Component Analysis. As the parameter we inserted numbers from k=1 to k=4 number of dimensions, and observed the results. Interestingly enough, the results in terms of  accuracy, precision and recall were all the same for different k number of dimensions, but changed with respect to each of these three DR methods. They however change for each k when other classifiers are applied, such as KNN, Linear Regression and Naïve Bayes. For example when we employ UFS with Linear Regression or Naïve Bayes, the results stay the same for k=1&2, but change for k=3&4. It is also important to mention that the UFS method selects Light when the k=1, Light and CO2 when the k=2, Light, CO2 and temperature when the k=3, Light, CO2, temperature and the humidity ratio when the k=4.Singular Value Decomposition and PCA, however, completely alter the attributes and yield a new single-or multi dimensional array (depending on the k given) instead of selecting the best attributes.
\\

\noindent The results obtained by the UFS and RFE analysis are identical to the dataset which is not reduced. It is interesting to observe that the RFE method delineated ‘Light’ as the only attribute worth keeping. The results obtained by the PCA are reported in figure 2 and the results obtained by SVD are reported in figure 3. Finally, we can conclude that, using the Decision Tree classifier, none of the methods yielded better results than the non-reduced dataset did. In order to complete the analysis, we should reproduce the results using all the four classifiers, but for the sake of keeping to the page limit of the report, we will not do that.


\begin{figure}[H] \centering
\includegraphics[width=8cm]{dimens_reduction/figure_2.png}
\includegraphics[width=8cm]{dimens_reduction/figure_3.png}
\caption{\label{fig:frog1}Figure 2 and 3}
\end{figure}


\noindent\textbf{PCA two component analysis tested on different classifiers}: In addition to experiments on different dimensionality reduction techniques, we tested the PCA analysis with two components (two dimensions) on Decision Tree (Figure 2), KNN(Figure 4), Naïve Bayes(Figure 5), and Logistic Regression(Figure 6). 

\begin{figure}[H] \centering
\includegraphics[width=8cm]{dimens_reduction/figure_4.png}
\includegraphics[width=8cm]{dimens_reduction/figure_5.png}
\includegraphics[width=8cm]{dimens_reduction/figure_6.png}
\caption{\label{fig:frog1}Figure 4, 5 and 6}
\end{figure}


\noindent The results obtained for the decision tree with two components are slightly worse with respect to the results obtained using the complete dataset. Accuracy with 2-component PCA for KNN remained the same, F1 with respect to the class 0 slightly improved whereas the F1 for the class 1 slightly worsened. The results worsened for the NB and, most importantily,  improved for the Logistic Regression when using only two components in the PCA with respect to applying the Logistic regression to all the attributes in the dataset. Interestingly enough, Logistic regression  with a two component PCA analysis  delivers precision, accuracy and recall results identical to the decision tree when applied to the whole dataset. We tested the results on the second test set (datatest2.txt) and the results improved even further (Figure 10).
Finally, in the Figure 7 we can observe the two dimensional representation of the dataset obtained with the PCA. Figure 8 shows the same, except for the fact that the training set has been reduced to 1000 samples instead of 8143, and the trend can thus be observed better. It is also interesting to observe the fraction of variance with respect to each of the  5 attributes (date excluded) in Figure 9.


\begin{figure}[H] \centering
\includegraphics[width=8cm]{dimens_reduction/figure_7.png}
\includegraphics[width=8cm]{dimens_reduction/figure_8.png}
\includegraphics[width=8cm]{dimens_reduction/figure_9.png}
\includegraphics[width=8cm]{dimens_reduction/figure_10.png}
\caption{\label{fig:frog1}Figure 7, 8, 9 and 10}
\end{figure}




\subsection{Imbalanced Learning}

Successivamente abbiamo verificato la composizione della variabile di output, Occupancy, la quale è così composta: per il valore "0" vi sono 6414 record su 8143 (ovvero il 79\%), invece per quanto riguarda il valore "1" vi sono 1729 record, il 21\%. Possiamo quindi osservare un notevole sbilanciamento verso il valore "0".
Abbiamo pertanto deciso d'incrementare questo sbilanciamento fino a ottenere uno squilibrio del 97\% per il valore "0", e avere così solo il 3\% di oggetti con valore "1".
Dopo aver provato le diverse tecniche abbiamo deciso di creare le seguenti tre diverse versioni sbilanciate del training set utilizzando diverse tecniche di undersampling e oversempling: 
\begin{enumerate}
    \item \textbf{Undersampled (che chiameremo \textit{dtU}):} la prima versione di data training è stata modificata attraverso una variante del modello "Near Neighbor", ovvero "Near Miss", decrementando così il numero di oggetti con valore "1" da 1729 fino a 198 record, e lasciando invariato il numero di record per il valore "0".
    \item \textbf{Oversampled (che chiameremo \textit{dtO}):} nel secondo abbiamo invece lasciato invariato il numero di oggetti con valore "1", ed incrementato il numero record per il valore "0" a 55904, in maniera tale che rappresentassero il 97\% del data training implementando il modello "SMOTE".
    \item \textbf{Undersampled e Oversampled (che chiameremo \textit{dtUO}):} la composizione dell'ultimo è stata modificata completamente ottenendo la seguente composizione: 55031 oggetti per il valore "0" e 1702 per l' "1" attraverso l'implementazione delle seguenti tecniche: "Random OverSampler" e "Neighbourhood Cleaning Rule".
\end{enumerate}


\subsubsection{Classificazioni base}
Procediamo ora ad applicare diverse tecniche di classificazione ai tre training set modificati. Essendo questi sbilanciati in maniera considerevole l'obiettivo è quello di ottenere un valore dell'accuracy almeno pari a classificare sempre con il valore "0", quindi del 97\%.
\subsubsection*{Decision tree}
Anche qui si è cercato di ottenere la migliore classificazione attraverso l'uso della Randomized Search, cercando quindi la migliore combinazione degli Iper-Parametri "min samples split"e "min samples leaf". Abbiamo effettuato diverse prove con il numero di iterazioni in un range tra i 150 e 600. I parametri ottenuti sono i seguenti: 

\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
& \textbf{Min samples split} & \textbf{Min samples leaf} \\
\hline\hline \centering
\textit{Undersampled} & 308 & 2 \\
\hline \centering
\textit{Oversampled} & 432 & 8 \\ 
\hline \centering
\textit{Undersampled e Oversampled} & 372 & 90 \\ 
\hline
\end{tabular}
\end{center}

\noindent Le classificazioni degli ultimi due training set (dtO e dtUO) hanno rispettivamente ottenuto nel primo test set una accuracy di 0.90994 e 0.91219, e nel secondo di 0.9477 e 0.96021. Non hanno quindi raggiunto il valore minimo accettabile. 
Osserviamo quindi i valori ottenuti con la prima classificazione, quella su dtU:

\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
& \textbf{Test Set 1} & \textbf{Test Set 2} \\
\hline\hline \centering
\textit{Accuracy:} & 0.97673 & 0.99292 \\
\hline \centering
\textit{F1-score:} & [0.98144, 0.96881] & [0.99551, 0.98324] \\ 
\hline \centering
\textit{Weighted Avg F1-score} & 0.98 & 0.99 \\ 
\hline
\textit{Precision:} & [0.99, 0.95] & [1.00, 0.98] \\ 
\hline \centering
\textit{Recall:} & [0.97, 0.99] & [0.99, 0.99] \\ 
\hline
\end{tabular}
\end{center}
I valori ottenuti nel secondo test set sono estremamente positivi, tra i migliori, ma comunque inferiori a quelli ottenuti con l'utilizzo dello stesso modello sul training originale.
Dai grafici sottostanti si può però osservare che nonostante l'accuracy del terzo training set sia inferiore a quella del primo quest'ultimo ha però un'aria sotto la curva maggiore in entrambi i test set. 

\begin{figure}[H]
  \centering
  \subfloat[AUC dell'undersampled data training  nei due test set.]{\includegraphics[width=0.385\textwidth]{imbalance/roctree.png}\label{fig:auc1}}
  \hspace{1.5cm}
  \subfloat[AUC dell'undersampled e oversampled data training  nei due test set.]{\includegraphics[width=0.385\textwidth]{imbalance/uoroctree.png}\label{fig:auc22}}
\end{figure}

\subsubsection*{K Nearest Neighbor, Logistic Regression}
Questi due modelli hanno valori dell'accuracy  nei due test set che partono da 0.91219 e arrivano ad un valore massimo di 0.96021 e non vi è nulla di interessante di cui discutere, possiamo però notare che la combinazione di PCA e Logistic Regression ha ottenuto uno tra i migliori risultati, applicando invece lo stesso modello sul training sbilanciato con tre tecniche differenti non si riesce ad ottiene un risultato ottimale.

\subsubsection*{Naive Bayes}
Risultati interessanti sono stati ottenuti con il classificatore "Naive Bayes", questo modello non richiedeva nessun Iper-Parametro in input quindi non si è ricorsi all'utilizzo del modello Randomized Search. Nella tabella sottostante è possibile osservare i risultati ottenuti dai tre data training.  

\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
& \textbf{Accuracy test set 1} & \textbf{Accuracy test set 2} \\
\hline\hline \centering
\textit{Undersampled} & 0.91744 & 0.95888 \\
\hline \centering
\textit{Oversampled} & 0.97711 & 0.98985 \\ 
\hline \centering
\textit{Undersampled e Oversampled} & 0.97711 & 0.97898 \\ 
\hline
\end{tabular}
\end{center}
E' curioso osservare che i dataset che riescono ad ottenere risultati ottimali utilizzando questo classificatore sono i training maggiormente modificati (dtO e dtUO), che hanno ottenuto valori positivi dell'accuracy nei precedenti modelli, ma che non riuscivano a raggiungere il valore minimo accettabile. I due data training modificati ottengono i medesimi risultati nel primo test set, migliori rispetto a quelli ottenuti dal Decision Tree, ma la classificazione ottenuta dal training dtO ha prodotto dei valori migliori nel secondo test set, ma sopratutto è uno dei modelli con la maggiore AUC per entrambi i test set.  


\begin{center}
\begin{tabular}{ |l|l|l|l|l| }
\hline & \multicolumn{2}{|c|}{ \textbf{Oversampled}} & \multicolumn{2}{|c|}{ \textbf{Under e Oversampled}}\\

\hline
& \textbf{Test Set 1} & \textbf{Test Set 2}& \textbf{Test Set 1} & \textbf{Test Set 2} \\
\hline\hline \centering
\textit{Accuracy:} & 0.97711 & 0.98985 & 0.97711 & 0.97897\\
\hline \centering
\textit{F1-score:} & [0.98172, 0.96939] & [0.99354, 0.9762 ] & [0.98172, 0.96939] & [0.986746, 0.94922]\\ 
\hline \centering
\textit{Weighted Avg F1-score} & 0.98 & 0.99 & 0.98 & 0.98\\ 
\hline
\textit{Precision:} & [1.00, 0.95] & [1.00, 0.96] & [1.00, 0.95] & [0.98, 0.96] \\ 
\hline \centering
\textit{Recall:} & [0.99, 0.99] & [0.99, 0.99] & [0.97, 0.99] & [0.99, 0.94]\\ 

\hline
\end{tabular}
\end{center}



\begin{figure}[H]
  \centering
  \subfloat[AUC dell'oversampled data training nei due test set.]{\includegraphics[width=0.385\textwidth]{imbalance/nbroc2.png}\label{fig:auc1}}
  \hspace{1.5cm}
  \subfloat[AUC dell'undersampled e oversampled data training nei due test set.]{\includegraphics[width=0.385\textwidth]{imbalance/nbroc3.png}\label{fig:auc22}}
\end{figure}


\section{Task 2}

\subsection{Support Vector Machines (SVM)}
\subsubsection{Linear SVM}
Per prima cosa abbiamo valutato la qualità della classificazione attraverso il classificatore Linear SMV attraverso una rappresentazione grafica dei valori Precision e Recall che è possibile ottenere nei due test set con il variare delle diversi parametri, e il valore medio assunto dalla Precision (AP) che riassume il grafico stesso. Come è possibile vedere nel primo test set la curva presenta diversi e profondi picchi, invece la seconda curva è più lineare.

\begin{figure}[H]
  \centering
  \subfloat[Precision-Recall curve per il primo test set.]{\includegraphics[width=0.385\textwidth]{SVM/PR1.png}\label{fig:auc1}}
  \hspace{1.5cm}
  \subfloat[Precision-Recall curve per il secondo test set.]{\includegraphics[width=0.385\textwidth]{SVM/PR2.png}\label{fig:auc22}}
\end{figure}

Successivamente abbiamo fatto diverse prove modificando i vari valori assunti dai parametri: penalty, loss, multi\_class, dual,fit\_intercept, tol e C.
\subsubsection{SVC}
\subsection{Ensemble Learning}

\textbf{Random Forest}: I Decision Boundary tipici che hanno restituito i risultati più stabili, apprezzabili dal valore degli indici di validazione sul Training Set e dell'area AUC sul Test Set, sono stati quelli della Logistic Regression e Naive Bayes. Tramite la Random Forest è stato posto l'obiettivo di raggiungere quella stabilità, tramite un Decision Boundary analogo, insieme ad una performance sui Test Set pari a quella ottenuta con il Decision Tree. Lo scopo è stato raggiunto con un modello che ha per Iper-Parametri: 'max features'='log2', 'random state'=0, 'n estimators'=100', 'max depth'=3 e i valori di default per gli altri. Il valore migliore della 'max depth' è stato trovato mediante la Random Search cercando nel dominio [1,100] e ottimizzando per l'F1 Score. Il fatto che il 'max depth' ottimale sia un valore basso indica che sono sufficienti singoli alberi semplici non overfittati. Le feature diverse da 'Light' aumentano la loro importanza. Il Decision Boundary è più preciso per valori bassi di 'CO2'. Quando 'CO2' è basso 'Light' viene splittato ad un valore leggermente superiore a 365.125, come si nota dall'albero proposto quì (split a 371.125). In questo modo la zona di maggior contatto tra gli addensamenti delle due classi in output viene separata con più precisione.

\begin{figure}[H] \centering
\includegraphics[width=6cm]{ensamble/Random Forest Plot.png}
\includegraphics[width=9cm]{ensamble/Feature Importance Random Forest.png}
\includegraphics[width=9cm]{ensamble/Decision Boundary Random Forest CO2 vs Light.png}
\end{figure}

\noindent La 5 Fold Cross-Validation sul Training Set ha restituito: Accuracy con confidenza del 95\% = 0.98 (+/- 0.03); F1 Score medio pesato con confidenza del 95\% = 0.98 (+/- 0.03). Con una varianza minore del Decision Tree semplice il modello risulta più stabile sul Training Set. Sui Test Set i risultati delle metriche di valutazione sono identici a quelli del Decision Tree iniziale. Migliora però l'AUC per entrambi i Test Set: AUC(Test 1)=0.986 AUC(Test 2)=0.996. Dnque il modello è più stabile anche sui Test Set e l'obiettivo è stato raggiunto (questo modello è leggermente migliore dello Stump sul dataset iniziale non trasformato).
\\


\noindent \textbf{Bagging}: Il Bagging è stato eseguito con lo stesso scopo della Random Forest, con classificatori diversi dal Decision Tree: Support Vector Machine, Naive Bayes, Logistic regression. Particolare attenzione è stata riservata a quest'ultimo perché era stato il modello più stabile nella fase di classificazione di base. I risultati migliori in assoluto fino ad ora infatti vengono ottenuti con un Bagging composto da 50 Logistic Regression (50 è il numero di base classifier minimo per raggiungere il risultato migliore). A fronte degli stessi risultati della Random Forest sia per la Cross-Validation che per i test abbiamo quì i migliori valori dell'AUC.

\begin{figure}[H] \centering
\includegraphics[width=7cm]{ensamble/ROC Curve Test 1 Bagging.png}
\includegraphics[width=7cm]{ensamble/ROC Curve Test 2 Bagging.png}
\end{figure}

\noindent I risultati con gli altri tipi di classificatori base sono tutti peggiori. La Regressione Logistica è risultato il modello migliore anche dopo aver ridotto il Dataset a due dimensioni con la PCA. Dunque è stato fatto un tentativo di classificazione con Bagging sulle Logistic Regressions anche con il dataset bidimensionale uscito dalla PCA. I risultati però sono stati lievemente peggiori di quelli discussi quì sopra.
\\

\noindent \textbf{Boosting}: Come è noto l'AdaBoost Classifier è un Ensamble method che si basa sulla maggior attenzione riservata agli oggetti misclassificati nelle iterazioni precedenti. A questo modello quindi è stato riservato lo scopo di produrre un Decision Boundary che individuasse l'area centrale dello Scatterplot dove è presente un piccolo addensamento di oggetti con label in outpu 0 che sono sempre stati classificati male. Essi potrebbero essere i False Positive che hanno tenuto lievemente più bassa la Precision in tutti i classificatori di base. Lo scopo è stato raggiunto con un modello che usa il Decision Tree come Base Classifier e che ha per Iper-Parametri: 'n estimators'=100, 'random state'=0 e 'learning rate'=0.2, quest'ultimo trovato ottimizzando l'F1 Score tramite una Grid Search nel dominio [0,1] con step 0.1.


\begin{wrapfigure}[13]{r}{0.5\textwidth}
\includegraphics[width=9cm]{ensamble/Decision Boundary AdaBoost Tree CO2 vs Light.png}
\end{wrapfigure}

\noindent Come si può notare dal Boundary vengono individuati in pratica tutti gli oggetti precedentemente misclassificati sul Training Set. L'esito sia in termini di Cross-Validation sul Training Set che in termini di Test peggiorano sensibilmente. Accuracy ed F1 Score medio pesato passano entrambi ad un valore di 0.93 con due deviazioni standard (95\% di confidenza) dell'ordine di 0.12. Sui Test Set l'Accuracy passa a 0.922 e 0.938 rispettivamente e la Positive Precision peggiora passando a 0.94 e 0.85 rispettivamente. Le AUC poi crollano a 0.905 e 0.91. Il modello è evidentemente overfittato sul Training Set e questo dimostra che gli oggetti al centro dello ScatterPlot, con 'Light' superiore a 600 e 'CO2' intorno a 1000 sono più un caso specifico presente nel Training Set fornito piuttosto che una relazione da cui poter derivare conclusioni sul generali.\newline
L'AdaBoost è stato utilizzato anche con un Naive Bayes Classifier che ha permesso di individuare come zona della classe 0 anche quella con 'CO2'>1200 e 'Light'>600. I risultati però sono stati altrettanto pessimi.


\subsection{Neural Networks and Deep Learning}

\noindent \textbf{Single Layer Perceptron}: Gli iper-parametri usati per ottenere i risultati migliori con la  funzione Perceptron sono stati scelti usando la funzione CrossValidationCV con l’obiettivo di miglioramento della misura F1. Nonostante la ottimizazzione del modello raggiunta con alpha uguale a 0.0281 e penalty uguale a L1 (regressione lasso) , i risultati ottenuti non si sono mostrati i migliori (Figure1).
\begin{wrapfigure}[9]{r}{0.5\textwidth}
\includegraphics[width=12cm]{NN/figure 1.png}
\end{wrapfigure}


\noindent \textbf{Multi Layer Perceptron}: Dopo multiple prove e sperimenti con i vari iper-parametri per multi layer perceptron, sono state scoperte parecchie cose interessanti. Ad esempio, se ci sono solo due hidden layer, non cambia molto i risultati se il primo layer è settato a 100 o a 200, o se il secondo è settato a 50 o a 90. 
Una delle prime prove con MLP era quella di settare il primo layer a 100, il secondo a 50, e il terzo a 25, scegliendo Adam come solver, l’alpha di 0.0001 la learning rate adattiva, e la funzione di attivazione tangente iperbolica con momentum di 0.9. Mentre la Loss usciva abbastanza bassa (0.01868), l’accuracy e F1 (0.92908 e 0.9457, 0.89778, rispettivamente)  invece usciva molto di meglio con rispetto alla Single Layer Perceptron, però molto di peggio con rispetto ai risultati migliori ottenuti finora con gli altri classifier.
Dopo tante prove, I risultati invece si sono migliorati scegliendo Stochastic Gradient Descent come solver al posto di adam, e riducendo il numero dei strati a 2, cioè, 100 e 50 per evitare l’overfitting della dataset. Un’altra cosa fatta con lo stesso scopo era alzare l’alpha che si aggiunge alla loss function a 0.01 per ‘penalizzare’ il modello per l’overfitting. Abbiamo di nuovo scelto L2 (lasso regression).  Nonostante l’alzamento della Loss corrente al 0.05 con rispetto al modello precedente, l’accuracy invece è saltata a 0.9786  e le misure F1 a 0.9829 e 0.97124, rispettivamente per la variabile indipendente (‘Occupancy’) di 0 e 1. Il grafico della loss function può essere osservato nella figura 2. 

\begin{figure}[H]\centering
\includegraphics[width=9cm]{NN/Figure 2.png}
\end{figure}


\section{Task 3}

Come dataset per questo task è stato usato il Training Set fornito. L'attributo 'date' è costituito da valori del tempo rilevati ogni minuto ma sono presenti diversi valori dove anziché all'istante del minuto preciso è stato osservato il valore un secondo prima (es. 17:51:59 anziché 17:52:00). Si tratta di un piccolo chiaro errore di rilevazione e quindi affinché le serie temporali generate risultassero coerenti tutti i valori con 59 secondi sono stati corretti mediante il loro Offset di 1 secondo in avanti.

\subsection{Forecasting}

Sono state osservate graficamente tutte le Time Series di ciascun attributo del dataset e quella che è stata scelta per eseguire questo task, perché risultata la più interessante, è stata quella relativa all'attributo 'Temperature'.

\begin{wrapfigure}[12]{r}{0.5\textwidth}
\includegraphics[width=10cm]{forecast/TS Temperature.png}
\end{wrapfigure}

\noindent La serie temporale presenta un leggero trend discendente dall'inizio fino al giorno 2015-02-08. Successivamente sembra stabilizzarsi o addirittura cominciare un inversione di trend. E' presente un'evidente stagionalità che si ripete ogni 1440 minuti, cioè esattamente ogni giorno. Ciò è dovuto all'escursione termica tra il giorno e la notte: è evidente anche in questo grafico che le temperature minime si hanno nelle ore notturne. La varianza della serie rimane abbastanza stabile al variare del tempo, ma per alcune giornate la differenza termica è importante, ad esempio tra il 2015-02-07 ed il 2015-02-08. Il Dickey-Fuller Test per la verifica della stazionarietà indica che la serie non è propriamente stazionaria: Test Statistic = -2.69; Critical Value (1\%) = -3.43; Critical Value (5\%) = -2.86; Critical Value (10\%) = -2.57. E' stato deciso quindi di renderla stazionaria riducendo la varianza ed eliminando il trend. La varianza è stata ridotta mediante una trasformazione logaritmica della Time Series (ciascun valore diventa il logaritmo naturale del valore originale) ed il trend è stato eliminato sottraendo a ciascun valore la media mobile a 300 periodi (cioè il valore medio della temperatura nelle 5 ore precedenti). La nuova serie è stazionaria: il Test Statistics del Dickey-Fuller Test passa a -3.88, inferiore a tutti i Critical Value per ogni confidenza (già riportati sopra). Dal grafico della Partial Autocorrelation si intuisce che la serie potrebbe essere approssimata da un modello autoregressivo a 4 periodi.


\begin{figure}[H]\centering
\includegraphics[width=9cm]{forecast/TS Temperature Stationary.png}
\includegraphics[width=8cm]{forecast/TS Temperature PACF.png}
\end{figure}

\noindent Per effettuare il forecasting la TS è stata suddivisa tenendo i primi 6129 valori come training e i successivi come test. In questo modo il test set comincerà esattamente alla mezzanotte del 2015-02-09.
\\

\noindent \textbf{Exponential Smoothing}: Data la stagionalità accentuata è stato prodotto un modello basato su 1440 periodi (1 giorno in minuti) in modo da catturare il pattern tipico della temperatura giornaliera.

\begin{figure}[H]\centering
\includegraphics[width=16cm]{forecast/Exponential Smoothing Forecast.png}
\end{figure}

\begin{center}
\begin{tabular}{ | m{2cm} | m{2cm}| m{2cm} | m{2cm}| m{2cm} | m{2cm}| m{2cm} | } 
\hline
R2 & MAE & MAD & MAPE & MAXAPE & TAPE\\
\hline\hline \centering
0.7698 & 0.0101 & 0.0078 & 1.24 & 62.82 & 2130.31 \\
\hline
\end{tabular}
\end{center}



\section{Task 4}


\section{Task 5}


\end{document}
