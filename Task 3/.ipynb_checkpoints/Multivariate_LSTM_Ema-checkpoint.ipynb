{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('training.csv')\n",
    "del df['date']\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4885, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, Conv2D, MaxPool2D, Flatten, Dropout, LeakyReLU, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4885, 5, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1629, 5, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(4885,1,5)\n",
    "X_test = X_test.reshape(1629,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  1\n",
      "N. LABELS (classes):  2\n",
      "N. FEATURES:  5\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_outputs, n_features = X_train.shape[1], len(np.unique(y_train)), X_train.shape[2] \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS (classes): \", n_outputs)\n",
    "print(\"N. FEATURES: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm2(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(2, input_shape=(n_timesteps, n_features), return_sequences=True, \n",
    "                        kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #1\n",
    "    for _ in range(2):\n",
    "        model.add(LSTM(2, kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.04))   \n",
    "\n",
    "    #2\n",
    "    model.add(LSTM(32, kernel_initializer='TruncatedNormal', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "    \n",
    "    #3\n",
    "    for _ in range(2):\n",
    "        model.add(Dense(256, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))\n",
    "    #4\n",
    "    for _ in range(1):\n",
    "        model.add(Dense(64, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.7))\n",
    "\n",
    "    #5\n",
    "    model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = build_lstm2(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 1, 2)              64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 1, 2)              8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 1, 2)              40        \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 1, 2)              8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 1, 2)              40        \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 1, 2)              8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1, 2)              0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 32)                4480      \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 100,042\n",
      "Trainable params: 98,750\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\email\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3908 samples, validate on 977 samples\n",
      "Epoch 1/50\n",
      "3908/3908 [==============================] - 14s 4ms/step - loss: 0.4234 - accuracy: 0.8191 - val_loss: 0.5198 - val_accuracy: 0.7861\n",
      "Epoch 2/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.2467 - accuracy: 0.9051 - val_loss: 0.8569 - val_accuracy: 0.7861\n",
      "Epoch 3/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.2181 - accuracy: 0.9161 - val_loss: 1.4595 - val_accuracy: 0.7861\n",
      "Epoch 4/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.2041 - accuracy: 0.9153 - val_loss: 0.1243 - val_accuracy: 0.9724\n",
      "Epoch 5/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1900 - accuracy: 0.9289 - val_loss: 0.0874 - val_accuracy: 0.9857\n",
      "Epoch 6/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1824 - accuracy: 0.9276 - val_loss: 0.1025 - val_accuracy: 0.9795\n",
      "Epoch 7/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1783 - accuracy: 0.9294 - val_loss: 0.0695 - val_accuracy: 0.9785\n",
      "Epoch 8/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1616 - accuracy: 0.9404 - val_loss: 0.0764 - val_accuracy: 0.9826\n",
      "Epoch 9/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1670 - accuracy: 0.9383 - val_loss: 0.0702 - val_accuracy: 0.9836\n",
      "Epoch 10/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1619 - accuracy: 0.9399 - val_loss: 0.0826 - val_accuracy: 0.9857\n",
      "Epoch 11/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1831 - accuracy: 0.9250 - val_loss: 0.0949 - val_accuracy: 0.9775\n",
      "Epoch 12/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1654 - accuracy: 0.9317 - val_loss: 0.1652 - val_accuracy: 0.9191\n",
      "Epoch 13/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1641 - accuracy: 0.9360 - val_loss: 0.1165 - val_accuracy: 0.9417\n",
      "Epoch 14/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1541 - accuracy: 0.9450 - val_loss: 0.0870 - val_accuracy: 0.9857\n",
      "Epoch 15/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1597 - accuracy: 0.9411 - val_loss: 0.0774 - val_accuracy: 0.9857\n",
      "Epoch 16/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1596 - accuracy: 0.9381 - val_loss: 0.1682 - val_accuracy: 0.9232\n",
      "Epoch 17/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1590 - accuracy: 0.9417 - val_loss: 0.0644 - val_accuracy: 0.9846\n",
      "Epoch 18/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1484 - accuracy: 0.9419 - val_loss: 0.1280 - val_accuracy: 0.9437\n",
      "Epoch 19/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1469 - accuracy: 0.9483 - val_loss: 0.0622 - val_accuracy: 0.9857\n",
      "Epoch 20/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1655 - accuracy: 0.9368 - val_loss: 0.2258 - val_accuracy: 0.8628\n",
      "Epoch 21/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1515 - accuracy: 0.9391 - val_loss: 0.2679 - val_accuracy: 0.9038\n",
      "Epoch 22/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1582 - accuracy: 0.9388 - val_loss: 0.0907 - val_accuracy: 0.9611\n",
      "Epoch 23/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1567 - accuracy: 0.9401 - val_loss: 0.1134 - val_accuracy: 0.9478\n",
      "Epoch 24/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1724 - accuracy: 0.9358 - val_loss: 0.7028 - val_accuracy: 0.7871\n",
      "Epoch 25/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1510 - accuracy: 0.9394 - val_loss: 0.4314 - val_accuracy: 0.7953\n",
      "Epoch 26/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1566 - accuracy: 0.9335 - val_loss: 0.0706 - val_accuracy: 0.9857\n",
      "Epoch 27/50\n",
      "3908/3908 [==============================] - 5s 1ms/step - loss: 0.1680 - accuracy: 0.9319 - val_loss: 0.0749 - val_accuracy: 0.9744\n",
      "Epoch 28/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1528 - accuracy: 0.9417 - val_loss: 0.0647 - val_accuracy: 0.9857\n",
      "Epoch 29/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1603 - accuracy: 0.9350 - val_loss: 0.0908 - val_accuracy: 0.9693\n",
      "Epoch 30/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1586 - accuracy: 0.9358 - val_loss: 0.0759 - val_accuracy: 0.9826\n",
      "Epoch 31/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1507 - accuracy: 0.9414 - val_loss: 0.0779 - val_accuracy: 0.9857\n",
      "Epoch 32/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1683 - accuracy: 0.9291 - val_loss: 0.0730 - val_accuracy: 0.9857\n",
      "Epoch 33/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1587 - accuracy: 0.9419 - val_loss: 0.4067 - val_accuracy: 0.8976\n",
      "Epoch 34/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1620 - accuracy: 0.9347 - val_loss: 0.4121 - val_accuracy: 0.8639\n",
      "Epoch 35/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1469 - accuracy: 0.9424 - val_loss: 0.1334 - val_accuracy: 0.9386\n",
      "Epoch 36/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1566 - accuracy: 0.9404 - val_loss: 0.0780 - val_accuracy: 0.9857\n",
      "Epoch 37/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1453 - accuracy: 0.9401 - val_loss: 0.0916 - val_accuracy: 0.9744\n",
      "Epoch 38/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1494 - accuracy: 0.9427 - val_loss: 0.1063 - val_accuracy: 0.9785\n",
      "Epoch 39/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1386 - accuracy: 0.9470 - val_loss: 0.0641 - val_accuracy: 0.9836\n",
      "Epoch 40/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1630 - accuracy: 0.9371 - val_loss: 0.0973 - val_accuracy: 0.9570\n",
      "Epoch 41/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1517 - accuracy: 0.9409 - val_loss: 0.0890 - val_accuracy: 0.9857\n",
      "Epoch 42/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1434 - accuracy: 0.9437 - val_loss: 0.0568 - val_accuracy: 0.9846\n",
      "Epoch 43/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1489 - accuracy: 0.9432 - val_loss: 0.3845 - val_accuracy: 0.7871\n",
      "Epoch 44/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1501 - accuracy: 0.9411 - val_loss: 0.0891 - val_accuracy: 0.9662\n",
      "Epoch 45/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1531 - accuracy: 0.9409 - val_loss: 0.0672 - val_accuracy: 0.9836\n",
      "Epoch 46/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1589 - accuracy: 0.9335 - val_loss: 0.1135 - val_accuracy: 0.9806\n",
      "Epoch 47/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1562 - accuracy: 0.9440 - val_loss: 0.1821 - val_accuracy: 0.9335\n",
      "Epoch 48/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1677 - accuracy: 0.9337 - val_loss: 0.0684 - val_accuracy: 0.9857\n",
      "Epoch 49/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1406 - accuracy: 0.9450 - val_loss: 0.0599 - val_accuracy: 0.9846\n",
      "Epoch 50/50\n",
      "3908/3908 [==============================] - 4s 1ms/step - loss: 0.1526 - accuracy: 0.9399 - val_loss: 0.0665 - val_accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "history_lstm2 = lstm2.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                          validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.988950276243094\n",
      "F1-score [0.99286846 0.97547684]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1270\n",
      "           1       0.95      1.00      0.98       359\n",
      "\n",
      "    accuracy                           0.99      1629\n",
      "   macro avg       0.98      0.99      0.98      1629\n",
      "weighted avg       0.99      0.99      0.99      1629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm2.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
